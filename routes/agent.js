const express = require('express');
const router = express.Router();
const { Pool } = require('pg');
const { Pinecone } = require('@pinecone-database/pinecone');
const OpenAI = require('openai');
require('dotenv').config();

const pool = new Pool({ connectionString: process.env.DATABASE_URL, ssl: { rejectUnauthorized: false } });
const pinecone = new Pinecone({ apiKey: process.env.PINECONE_API_KEY });
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const pineconeIndex = pinecone.index(process.env.PINECONE_INDEX_NAME);
const maxLength = 16000; // charactersï¼Œé€‚å½“é™åˆ¶
//const truncatedText = conversationText.slice(-maxLength); // ä¿ç•™æœ€æ–°å†…å®¹
const personalitySummary = await generatePersonalitySummary(conversationText);
// AI åˆ†èº«å›å¤æ¥å£
router.post('/respond', async (req, res) => {
    const { user_id, message } = req.body;
    if (!user_id || !message) {
        return res.status(400).json({ error: 'user_id and message required' });
    }

    try {
        // 1. ç”Ÿæˆè¾“å…¥å‘é‡
        const embedResp = await openai.embeddings.create({
            model: "text-embedding-3-small",
            input: message
        });
        const inputVector = embedResp.data[0].embedding;

        // 2. æŸ¥è¯¢ç”¨æˆ·äººæ ¼å‘é‡ï¼ˆæœ€æ–°çš„ä¸€æ¡ï¼‰
        const vectorQuery = await pineconeIndex.query({
            vector: inputVector,
            topK: 1,
            filter: { user_id: user_id.toString(), source: "chat_history" },
            includeMetadata: true
        });

        const vector = vectorQuery.matches?.[0];
        const createdAt = vector?.metadata?.created_at || "";
        const personalitySummary = vector?.metadata?.summary || "The user is introspective, likes basketball, enjoys meaningful conversations.";

        // 3. æ„é€  system prompt
        const systemPrompt = `
You are a digital twin of user ${user_id}. 
Respond exactly as they would â€” using their tone, preferences, and emotional style.

Personality Summary (from chat history as of ${createdAt}):
${personalitySummary}

Never break character. Respond like a second brain or AI version of the user.
        `.trim();

        // 4. è·å–æœ€è¿‘ 6 è½®ä¸Šä¸‹æ–‡ï¼ˆæŒ‰æ—¶é—´å‡åºï¼‰
        const chatHistory = await pool.query(`
            SELECT sender, message FROM chat_history
            WHERE user_id = $1 ORDER BY created_at DESC LIMIT 6
        `, [user_id]);

        const contextMessages = chatHistory.rows.reverse().map(row => ({
            role: row.sender === 'user' ? 'user' : 'assistant',
            content: row.message
        }));

        const messages = [
            { role: "system", content: systemPrompt },
            ...contextMessages,
            { role: "user", content: message }
        ];

        // 5. è°ƒç”¨ ChatGPT ç”Ÿæˆå›å¤
        const response = await openai.chat.completions.create({
            model: "gpt-4o",
            messages
        });

        const reply = response.choices[0].message.content;

        // 6. å­˜å…¥ chat_history
        await pool.query(`
            INSERT INTO chat_history (user_id, sender, message)
            VALUES ($1, 'user', $2), ($1, 'ai', $3)
        `, [user_id, message, reply]);

        // 7. æ¯ 20 è½®æ›´æ–°ä¸€æ¬¡äººæ ¼å‘é‡
        const countResult = await pool.query(
            `SELECT COUNT(*) FROM chat_history WHERE user_id = $1`,
            [user_id]
        );
        const messageCount = parseInt(countResult.rows[0].count, 10);

        if (messageCount % 20 === 0) {
            const allChats = await pool.query(
                `SELECT sender, message FROM chat_history WHERE user_id = $1 ORDER BY created_at ASC`,
                [user_id]
            );
            const conversationText = allChats.rows.map(row =>
                `${row.sender === 'user' ? 'User' : 'AI'}: ${row.message}`
            ).join('\n').slice(-16000); // truncate

            const embeddingResponse = await openai.embeddings.create({
                model: "text-embedding-3-small",
                input: conversationText
            });

            const vector = embeddingResponse.data[0].embedding;

            await pineconeIndex.upsert([
                {
                    id: `chat_summary_${user_id}_${Date.now()}`,
                    values: vector,
                    metadata: {
                        user_id: user_id.toString(),
                        source: "chat_history",
                        created_at: new Date().toISOString(),
                        summary: personalitySummary // å¯ä»¥è‡ªåŠ¨ç”Ÿæˆ
                    }
                }
            ]);
            console.log(`âœ… Updated personality embedding for user ${user_id}`);
        }

        res.json({ reply });

    } catch (error) {
        console.error("ğŸ”¥ AI Agent error:", error);
        res.status(500).json({ error: "AI agent failed to respond." });
    }
});

router.get('/history', async (req, res) => {
    const { user_id } = req.query;
    if (!user_id) return res.status(400).json({ error: "Missing user_id" });

    try {
        const result = await pool.query(
            `SELECT sender, message, created_at FROM chat_history WHERE user_id = $1 ORDER BY created_at ASC`,
            [user_id]
        );
        res.json(result.rows);
    } catch (err) {
        console.error("Fetch history failed:", err);
        res.status(500).json({ error: "Failed to fetch history" });
    }
});
// ç”¨å†å²èŠå¤©æ›´æ–°äººæ ¼å‘é‡
router.post('/update-embedding-from-history', async (req, res) => {
    const { user_id } = req.body;
    if (!user_id) return res.status(400).json({ error: "Missing user_id" });

    try {
        // 1. è·å–æ‰€æœ‰èŠå¤©è®°å½•
        const allChats = await pool.query(
            `SELECT sender, message FROM chat_history WHERE user_id = $1 ORDER BY created_at ASC`,
            [user_id]
        );

        if (allChats.rows.length === 0) {
            return res.status(404).json({ error: "No chat history found for user" });
        }

        // 2. æ‹¼æ¥ä¸ºå¤§æ–‡æœ¬
        const conversationText = allChats.rows.map(row =>
            `${row.sender === 'user' ? 'User' : 'AI'}: ${row.message}`
        ).join('\n');

        // 3. è·å– embedding
        const embeddingResponse = await openai.embeddings.create({
            model: "text-embedding-3-small",
            input: conversationText
        });

        const vector = embeddingResponse.data[0].embedding;

        // 4. å­˜å…¥ Pinecone
        await pineconeIndex.upsert([
            {
                id: `chat_summary_${user_id}_${Date.now()}`,
                values: vector,
                metadata: {
                    user_id: user_id.toString(),
                    source: "chat_history",
                    created_at: new Date().toISOString()
                }
            }
        ]);

        res.json({ message: "Chat history embedded and stored in Pinecone." });
    } catch (error) {
        console.error("Embedding from history failed:", error);
        res.status(500).json({ error: "Failed to update embedding from history." });
    }
});

async function generatePersonalitySummary(conversationText) {
    const messages = [
        {
            role: "system",
            content: "You're an AI analyst summarizing a user's personality based on their chat history. Return 3-5 bullet points capturing their key interests, tone, social style, and preferences."
        },
        {
            role: "user",
            content: conversationText.slice(-16000) // é¿å…è¶…é•¿æ–‡æœ¬
        }
    ];

    const completion = await openai.chat.completions.create({
        model: "gpt-4o",
        messages
    });

    return completion.choices[0].message.content;
}

// åœ¨ /respond çš„æ›´æ–° embedding éƒ¨åˆ†ï¼Œè°ƒç”¨è¿™ä¸ªå‡½æ•°ç”Ÿæˆ summaryï¼š
if (messageCount % 20 === 0) {
    const allChats = await pool.query(
        `SELECT sender, message FROM chat_history WHERE user_id = $1 ORDER BY created_at ASC`,
        [user_id]
    );
    const conversationText = allChats.rows.map(row =>
        `${row.sender === 'user' ? 'User' : 'AI'}: ${row.message}`
    ).join('\n').slice(-16000);

    const embeddingResponse = await openai.embeddings.create({
        model: "text-embedding-3-small",
        input: conversationText
    });
    const vector = embeddingResponse.data[0].embedding;

    // ğŸ§  è‡ªåŠ¨ç”Ÿæˆäººæ ¼æ€»ç»“ï¼ˆæ³¨å…¥ metadataï¼‰
    const personalitySummary = await generatePersonalitySummary(conversationText);

    await pineconeIndex.upsert([
        {
            id: `chat_summary_${user_id}_${Date.now()}`,
            values: vector,
            metadata: {
                user_id: user_id.toString(),
                source: "chat_history",
                created_at: new Date().toISOString(),
                summary: personalitySummary
            }
        }
    ]);
    console.log("âœ… Updated personality with summary:\n", personalitySummary);
}

module.exports = router;
